{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4UHCwXIirz02qgKDor4aO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuat-arslan/NLP_Course/blob/main/Code_Final_NLP_Assignment3_Collocations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYfk9DxJYma3",
        "outputId": "da3faa3e-a9c7-4b8f-f36a-965621913b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of tokens (1425758,)\n",
            "Example postags [('part', 'NOUN'), ('i', 'VERB'), ('chapter', 'NOUN'), ('i', 'NOUN'), ('on', 'ADP'), ('an', 'DET'), ('exceptionally', 'ADV'), ('hot', 'ADJ'), ('evening', 'VERB'), ('early', 'ADJ')]\n",
            "example lemmatized vs normal: ['obliged', 'to', 'pass', 'her', 'kitchen', ',', 'the', 'door', 'of'] ['obliged', 'to', 'pass', 'her', 'kitchen', ',', 'the', 'door', 'of']\n",
            "Total number of tokens:  1425758\n",
            "Number of \"that\":  19429\n",
            "Number of \"the\":  48392\n",
            "Number of \"abject\":  21\n",
            "Number of \"london\":  2\n",
            "Number of \".\":  51738\n",
            "Number of (“magnificent”,“capital”) with window size 1 = 1\n",
            "Number of (“bright”,\"fire\") with window size 3 = 1\n",
            "Is Mr skimpole exist:  False\n",
            "Is spontaneous combustion exist:  False\n",
            "Number of occurance of spontaneous combustion: 1\n",
            "t-test with window size 1\n",
            "                     Bigram   t-score c(w1w2) c(w1) c(w2)\n",
            "0       stepan trofimovitch 22.619077     512   525   513\n",
            "1        pyotr stepanovitch 22.547839     509   834   509\n",
            "2          varvara petrovna 20.534441     422   474   507\n",
            "3         katerina ivanovna 20.239072     410   427   635\n",
            "4   nikolay vsyevolodovitch 17.657110     312   518   312\n",
            "5         fyodor pavlovitch 17.052928     291   306   461\n",
            "6                   old man 16.857569     289  1356  2546\n",
            "7       nastasia philipovna 15.583753     243   417   251\n",
            "8                 young man 15.140575     232   776  2546\n",
            "9                 old woman 14.353166     208  1356  1047\n",
            "10         yulia mihailovna 14.175303     201   215   202\n",
            "11         pyotr petrovitch 13.100118     172   834   331\n",
            "12    lizabetha prokofievna 13.074945     171   185   177\n",
            "13               great deal 12.790651     164  1202   237\n",
            "14      dmitri fyodorovitch 12.720232     162   427   327\n",
            "15       evgenie pavlovitch 12.563970     158   227   461\n",
            "16          thousand rouble 11.980517     144   614   543\n",
            "17                long time 11.793035     143  1074  2623\n",
            "18     mavriky nikolaevitch 11.487929     132   149   132\n",
            "19               first time 11.192482     130  1297  2623\n",
            "chi-test with window size 1\n",
            "                    Bigram      chi-score c(w1w2) c(w1) c(w2)\n",
            "0      stepan trofimovitch 1387729.399419     512   525   513\n",
            "1     ippolit kirillovitch 1359441.767387      41    43    41\n",
            "2        lef nicolaievitch 1359441.767387      41    43    41\n",
            "3        avdotya romanovna 1341883.293600     112   119   112\n",
            "4         yulia mihailovna 1326305.255085     201   215   202\n",
            "5          nikodim fomitch 1316082.461507      24    24    26\n",
            "6    lizabetha prokofievna 1273170.745295     171   185   177\n",
            "7     mavriky nikolaevitch 1263072.562364     132   149   132\n",
            "8      trifon borissovitch 1235651.733191      39    45    39\n",
            "9       rodion romanovitch 1205267.277621      82    97    82\n",
            "10      mihail makarovitch 1197633.359951      21    25    21\n",
            "11  gavrila ardalionovitch 1173527.381984      58    61    67\n",
            "12        arina prohorovna 1120124.358215      39    44    44\n",
            "13        varvara petrovna 1056419.206291     422   474   507\n",
            "14     semyon yakovlevitch 1034363.293854      37    51    37\n",
            "15          kuzma kuzmitch  983275.172327      20    29    20\n",
            "16        daria alexeyevna  980371.988481      19    21    25\n",
            "17          darya pavlovna  935805.629431      49    62    59\n",
            "18       katerina ivanovna  883756.256081     410   427   635\n",
            "19      pyotr stepanovitch  869958.438755     509   834   509\n",
            "likelihood ratio test with window size 1\n",
            "                     Bigram  loglikelihood-score c(w1w2) c(w1) c(w2)\n",
            "0         avdotya romanovna          1484.988324     112   119   112\n",
            "1        rodion romanovitch          1484.485695      82    97    82\n",
            "2      mavriky nikolaevitch          1484.274974     132   149   132\n",
            "3        andrey antonovitch          1483.464043      83   142    83\n",
            "4                 de griers          1482.991266      95   243    95\n",
            "5      nikolay parfenovitch          1482.609155     103   518   103\n",
            "6       stepan trofimovitch          1482.524036     512   525   513\n",
            "7          yulia mihailovna          1482.442392     201   215   202\n",
            "8   nikolay vsyevolodovitch          1482.215735     312   518   312\n",
            "9        pyotr stepanovitch          1481.769448     509   834   509\n",
            "10    lizabetha prokofievna          1480.829440     171   185   177\n",
            "11   pulcheria alexandrovna          1480.272014     123   124   242\n",
            "12          madame hohlakov          1479.994841      90   247    93\n",
            "13      nastasia philipovna          1478.483150     243   417   251\n",
            "14        fyodor pavlovitch          1477.400558     291   306   461\n",
            "15        katerina ivanovna          1476.984074     410   427   635\n",
            "16         varvara petrovna          1476.922372     422   474   507\n",
            "17               great deal          1475.957207     164  1202   237\n",
            "18      alexey fyodorovitch          1475.772277     106   226   327\n",
            "19       evgenie pavlovitch          1475.616140     158   227   461\n",
            "t-test with window size 3\n",
            "                     Bigram   t-score c(w1w2) c(w1) c(w2)\n",
            "0       stepan trofimovitch 22.602388     512   525   513\n",
            "1        pyotr stepanovitch 22.521453     509   834   509\n",
            "2          varvara petrovna 20.518038     422   474   507\n",
            "3         katerina ivanovna 20.220295     410   427   635\n",
            "4   nikolay vsyevolodovitch 17.644282     312   518   312\n",
            "5         fyodor pavlovitch 17.041334     291   306   461\n",
            "6                   old man 16.602824     290  1356  2546\n",
            "7       nastasia philipovna 15.574340     243   417   251\n",
            "8                 young man 15.025308     234   776  2546\n",
            "9                 old woman 14.459155     215  1356  1047\n",
            "10         yulia mihailovna 14.171011     201   215   202\n",
            "11    lizabetha prokofievna 13.071437     171   185   177\n",
            "12         pyotr petrovitch 13.070596     172   834   331\n",
            "13               great deal 12.798577     165  1202   237\n",
            "14      dmitri fyodorovitch 12.704848     162   427   327\n",
            "15       evgenie pavlovitch 12.552296     158   227   461\n",
            "16                    ha ha 12.519309     157   252   252\n",
            "17          thousand rouble 12.473985     157   614   543\n",
            "18           hundred rouble 12.329791     153   428   543\n",
            "19                   let go 11.689340     145  1085  1858\n",
            "chi-test with window size 3\n",
            "                    Bigram     chi-score c(w1w2) c(w1) c(w2)\n",
            "0      stepan trofimovitch 461893.806629     512   525   513\n",
            "1        lef nicolaievitch 453092.589206      41    43    41\n",
            "2     ippolit kirillovitch 453092.589206      41    43    41\n",
            "3        avdotya romanovna 447145.098601     112   119   112\n",
            "4         yulia mihailovna 441833.754547     201   215   202\n",
            "5          nikodim fomitch 438662.153881      24    24    26\n",
            "6    lizabetha prokofievna 424162.251792     171   185   177\n",
            "7     mavriky nikolaevitch 420848.189555     132   149   132\n",
            "8      trifon borissovitch 411831.911283      39    45    39\n",
            "9       rodion romanovitch 401646.427025      82    97    82\n",
            "10      mihail makarovitch 399183.120062      21    25    21\n",
            "11  gavrila ardalionovitch 391098.461363      58    61    67\n",
            "12        arina prohorovna 373322.786483      39    44    44\n",
            "13        varvara petrovna 351577.131206     422   474   507\n",
            "14     semyon yakovlevitch 344738.431769      37    51    37\n",
            "15          kuzma kuzmitch 327731.724277      20    29    20\n",
            "16        daria alexeyevna 326765.329658      19    21    25\n",
            "17                wisp tow 323415.555632      14    18    16\n",
            "18          darya pavlovna 311869.877774      49    62    59\n",
            "19       katerina ivanovna 294038.852117     410   427   635\n",
            "likelihood ratio test with window size 3\n",
            "                     Bigram  loglikelihood-score c(w1w2) c(w1) c(w2)\n",
            "0         avdotya romanovna          1475.449377     112   119   112\n",
            "1      mavriky nikolaevitch          1475.092831     132   149   132\n",
            "2     lizabetha prokofievna          1474.542984     171   185   177\n",
            "3    pulcheria alexandrovna          1474.390261     123   124   242\n",
            "4          yulia mihailovna          1474.269767     201   215   202\n",
            "5                     ha ha          1473.984641     157   252   252\n",
            "6                great deal          1473.841844     165  1202   237\n",
            "7       nastasia philipovna          1473.692388     243   417   251\n",
            "8       dmitri fyodorovitch          1473.543022     162   427   327\n",
            "9          pyotr petrovitch          1473.416282     172   834   331\n",
            "10       evgenie pavlovitch          1473.293795     158   227   461\n",
            "11  nikolay vsyevolodovitch          1473.248923     312   518   312\n",
            "12           hundred rouble          1473.002539     153   428   543\n",
            "13        fyodor pavlovitch          1472.915994     291   306   461\n",
            "14         varvara petrovna          1472.508886     422   474   507\n",
            "15      stepan trofimovitch          1472.424324     512   525   513\n",
            "16       pyotr stepanovitch          1472.273630     509   834   509\n",
            "17        katerina ivanovna          1472.263746     410   427   635\n",
            "18                  old man          1470.707417     290  1356  2546\n",
            "19                young man          1457.108826     234   776  2546\n",
            "for Head Clerk\n",
            "T-test:  4.674127666133668 Collocation? True\n",
            "Chi-square Test:  6294.821125652069 Collocation? True\n",
            "Likelihood Ratio Test:  209.66265578477234 Collocation? True\n",
            "for great man\n",
            "T-test:  3.73672357664933 Collocation? True\n",
            "Chi-square Test:  117.4030889458421 Collocation? True\n",
            "Likelihood Ratio Test:  45.15878843720214 Collocation? True\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"NLP_ASS3v5_final.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1nEM3ad9fSTR7cAomsUBSeoRF3Kf2XVhR\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "if 'Fyodor Dostoyevski Processed.txt' not in os.listdir():\n",
        "    print('Please upload the document Fyodor Dostoyevski Processed.txt')\n",
        "    uploaded = files.upload()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt') #required for tokenizer\n",
        "nltk.download('averaged_perceptron_tagger') #required for pos_tag\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "import pandas as pd\n",
        "pd.set_option('display.float_format', '{:.6f}'.format)\n",
        "\n",
        "\n",
        "# Load the stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "\n",
        "\"\"\"Read the txt file and tokenize it with nltk\"\"\"\n",
        "\n",
        "with open('/content/Fyodor Dostoyevski Processed.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "tokens = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "print('number of tokens', np.array(tokens).shape) #total number of tokens\n",
        "\n",
        "\"\"\"POS tags\"\"\"\n",
        "\n",
        "pos_tags = nltk.tag.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "#Example tags\n",
        "print('Example postags', pos_tags[0:10])\n",
        "\n",
        "\"\"\"Custom Lemantizer class\"\"\"\n",
        "\n",
        "class custom_lemmatizer:\n",
        "\n",
        "    tag_dict = {\n",
        "        \"ADJ\": wordnet.ADJ,\n",
        "        \"NOUN\": wordnet.NOUN,\n",
        "        # \"VERB\": wordnet.VERB,\n",
        "        # \"ADV\": wordnet.ADV\n",
        "    }\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    #Retured value is lower case.\n",
        "    def lemmatize(self, word_pos_tuple):\n",
        "        word = word_pos_tuple[0]\n",
        "        pos_tag = word_pos_tuple[1]\n",
        "        if pos_tag in self.tag_dict:\n",
        "            return self.lemmatizer.lemmatize(word, self.tag_dict[pos_tag]).lower()\n",
        "        else:\n",
        "            return word.lower()\n",
        "\n",
        "lemmatizer = custom_lemmatizer()\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(pos_tag) for pos_tag in pos_tags]\n",
        "\n",
        "#obliged mapped to oblige @100\n",
        "print('example lemmatized vs normal:', lemmatized_tokens[100:109], tokens[100:109])\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"#Candidate Collocation Generator\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def filter_bigrams(lemmatized_tokens, pos_tags, window_size,threshold):\n",
        "    filtered_bigrams = []\n",
        "    bigrams = []\n",
        "    for i in range(len(lemmatized_tokens) - 1):\n",
        "        for j in range(window_size):\n",
        "            try:\n",
        "                current_word = lemmatized_tokens[i]\n",
        "                next_word = lemmatized_tokens[i+j + 1]\n",
        "                current_pos_tag = pos_tags[i][1]\n",
        "                next_pos_tag = pos_tags[i+j + 1][1]\n",
        "                #print(current_word,next_word,current_pos_tag,next_pos_tag)\n",
        "\n",
        "                # Check if both words consist only of alphabetical characters and are not stopwords\n",
        "                if current_word.isalpha() and next_word.isalpha() and current_word not in stopwords_list and next_word not in stopwords_list:\n",
        "                    if (current_pos_tag == 'NOUN' and next_pos_tag == 'NOUN') or (current_pos_tag == 'ADJ' and next_pos_tag == 'NOUN'):\n",
        "                        filtered_bigrams.append((current_word, next_word))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "    # bigram_counts = Counter(filtered_bigrams)\n",
        "    # #print(filtered_bigrams)\n",
        "    # for bigram, count in bigram_counts.items():\n",
        "    #     if count >= threshold:\n",
        "    #         bigrams.append(bigram)\n",
        "\n",
        "    collocations = []\n",
        "\n",
        "    for i in range(len(lemmatized_tokens) - 1):\n",
        "        for j in range(window_size):\n",
        "            try:\n",
        "                collocation = (lemmatized_tokens[i], lemmatized_tokens[i+j+1])\n",
        "\n",
        "                collocations.append(collocation)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    bigram_counts = Counter(collocations)\n",
        "    filt_bigram_counts = Counter(filtered_bigrams)\n",
        "    for bigram, count in filt_bigram_counts.items():\n",
        "        if bigram_counts[bigram] >= threshold:\n",
        "            bigrams.append(bigram)\n",
        "\n",
        "\n",
        "\n",
        "    return bigrams\n",
        "\n",
        "filtered_bigram_list1 = filter_bigrams(lemmatized_tokens,pos_tags,1,10)\n",
        "#filtered_bigram_list3 = filter_bigrams(lemmatized_tokens,pos_tags,3,10)\n",
        "\n",
        "\n",
        "\"\"\"#Collecation list\"\"\"\n",
        "\n",
        "def generate_collocations(tokens, window_size):\n",
        "    collocations = []\n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "        for j in range(window_size):\n",
        "            try:\n",
        "                collocation = (tokens[i], tokens[i+j+1])\n",
        "\n",
        "                collocations.append(collocation)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return collocations\n",
        "\n",
        "\"\"\"#Probs\n",
        "\n",
        "in this part marginal probs will be counted as well as bigram probs by just Counts/Total (frequentiest approach)\n",
        "\"\"\"\n",
        "\n",
        "from scipy.stats import t as t_dist\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import chi2, binom\n",
        "\n",
        "class test_stats_calculator:\n",
        "    def __init__(self, tokens, window_size=3):\n",
        "        self.tokens = tokens\n",
        "        self.coll = self.generate_collocations(tokens,window_size)\n",
        "        self.token_counter = Counter(self.tokens)\n",
        "        self.coll_counter = Counter(self.coll)\n",
        "        self.window_size = window_size\n",
        "        self.N = len(tokens)*self.window_size\n",
        "\n",
        "    #Bigram generator.\n",
        "    def generate_collocations(self, tokens, window_size):\n",
        "        collocations = []\n",
        "\n",
        "        for i in range(len(tokens) - 1):\n",
        "            for j in range(window_size):\n",
        "                try:\n",
        "                    collocation = (tokens[i], tokens[i+j+1])\n",
        "\n",
        "                    collocations.append(collocation)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return collocations\n",
        "\n",
        "\n",
        "\n",
        "    def calc_prob(self, itm):\n",
        "\n",
        "        #sample item\n",
        "        if isinstance(itm, str):\n",
        "            #print('Calculating single element probablity...')\n",
        "            if itm in self.token_counter:\n",
        "                return self.token_counter[itm]/sum(self.token_counter.values())\n",
        "            else:\n",
        "                print('Input does not exist in the text!')\n",
        "                return\n",
        "\n",
        "        elif isinstance(itm, tuple):\n",
        "            #print('Calculating collocations probablity...')\n",
        "            #Number of collaction occurance divided by total numbe of tokens.\n",
        "            if itm in self.coll_counter:\n",
        "                return self.coll_counter[itm]/sum(self.coll_counter.values())\n",
        "            else:\n",
        "                print('Input collocation does not exist in the text!')\n",
        "                return\n",
        "\n",
        "        else:\n",
        "            raise TypeError('This type cannot be handeled')\n",
        "\n",
        "\n",
        "    '''\n",
        "    t_distribution will use infinite degree of freedom beacuse N is ver large\n",
        "    Then t dist. will approach to normal distribution. Also due to the lecture\n",
        "    notes I used one sided version.\n",
        "    '''\n",
        "    def t_dist_test(self, collocation, alpha=0.005):\n",
        "        #for H0\n",
        "        prob0 = self.calc_prob(collocation[0])\n",
        "        prob1 = self.calc_prob(collocation[1])\n",
        "\n",
        "        assert prob0 is not None, 'input words does not exist'\n",
        "        assert prob1 is not None, 'input words does not exist'\n",
        "        #H0: items are indipendent\n",
        "        H0 = prob0 * prob1\n",
        "\n",
        "        p = self.calc_prob(collocation)\n",
        "\n",
        "        assert p is not None, 'collocation does not exist'\n",
        "\n",
        "        S2 = p #p*(1-p)\n",
        "\n",
        "        t_val = (p - H0)/math.sqrt(S2/self.N)\n",
        "\n",
        "        if t_val > norm.ppf(1 - alpha):\n",
        "            return True, t_val #It is collocation\n",
        "        else:\n",
        "            return False, t_val\n",
        "\n",
        "\n",
        "\n",
        "    def chi_square_test(self,collocation, alpha=0.005):\n",
        "        observed = np.zeros((2,2))\n",
        "        expected = np.zeros((2,2))\n",
        "\n",
        "        if not isinstance(collocation, tuple):\n",
        "            raise TypeError('This type cannot be handeled')\n",
        "\n",
        "        if collocation not in self.coll_counter:\n",
        "            print('This collocation does not exist')\n",
        "            return 0\n",
        "\n",
        "        #print(self.coll_counter[collocation])\n",
        "        observed[0,0] = self.coll_counter[collocation]\n",
        "        observed[1,0] = self.token_counter[collocation[0]]*self.window_size - observed[0,0]\n",
        "        observed[0,1] = self.token_counter[collocation[1]]*self.window_size - observed[0,0]\n",
        "        observed[1,1] = self.N -  self.token_counter[collocation[0]]*self.window_size - observed[0,1]\n",
        "\n",
        "\n",
        "        #Generic part did not worked well\n",
        "        # expected[0,0] = (self.token_counter[collocation[0]]* self.token_counter[collocation[1]])/self.N\n",
        "        # expected[1,0] = (self.token_counter[collocation[0]]* (self.N - self.token_counter[collocation[1]]))/self.N\n",
        "        # expected[0,1] = ((self.N - self.token_counter[collocation[0]])* (self.token_counter[collocation[1]]))/self.N\n",
        "        # expected[1,1] = ((self.N - self.token_counter[collocation[0]])* (self.N - self.token_counter[collocation[1]]))/self.N\n",
        "\n",
        "\n",
        "\n",
        "        # expected = expected + 1 #add one smoothing\n",
        "        # chi_val = 0\n",
        "\n",
        "        # for i in range(observed.shape[0]):\n",
        "        #     for j in range(observed.shape[1]):\n",
        "        #         #print((observed[i,j], expected[i,j]),chi_val)\n",
        "        #         chi_val += (observed[i,j] - expected[i,j])**2 /expected[i,j]\n",
        "\n",
        "        #Shorcut worked better\n",
        "        chi_val = (self.N*(observed[0,0]*observed[1,1]-observed[0,1]*observed[1,0])**2)/(\n",
        "            (observed[0,0]+observed[0,1])*(observed[0,0]+observed[1,0])*\n",
        "             (observed[0,1]+observed[1,1])*(observed[1,0]+observed[1,1]))\n",
        "\n",
        "        df = (observed.shape[0] - 1)*(observed.shape[1] - 1)\n",
        "\n",
        "        if chi_val > chi2.ppf(1 - alpha,df):\n",
        "            return True, chi_val #It is collocation\n",
        "        else:\n",
        "            return False, chi_val\n",
        "\n",
        "\n",
        "\n",
        "    def loglikelihood_test(self,collocation, alpha=0.005):\n",
        "        eps = 5e-324 #math.ulp(0.0) value\n",
        "        c1 = self.token_counter[collocation[0]]*self.window_size\n",
        "        c2 = self.token_counter[collocation[1]]*self.window_size\n",
        "        c12 = self.coll_counter[collocation]\n",
        "        p = c2/self.N\n",
        "        p1 = c12/(c1)\n",
        "        p2 = (c2 - c12) / (self.N-c1)\n",
        "\n",
        "        LH1 = (binom.pmf(c12,c1,p))*(binom.pmf(c2-c12,self.N-c1,p))\n",
        "        LH2 = (binom.pmf(c12,c1,p1))*(binom.pmf(c2-c12,self.N-c1,p2))\n",
        "\n",
        "        if LH1 == 0:\n",
        "            LH1 = eps\n",
        "        if LH2 == 0:\n",
        "            LH2 = eps\n",
        "\n",
        "        log_val = -2*np.log((LH1/LH2))\n",
        "\n",
        "        if log_val > chi2.ppf(1 - alpha,1):\n",
        "            return True, log_val #It is collocation\n",
        "        else:\n",
        "            return False, log_val\n",
        "\n",
        "\"\"\"#Report top k\"\"\"\n",
        "\n",
        "def report_topk(k, lemmatized_tokens, pos_tags, test='t_test', window_size = 3, filter_freq=10, ):\n",
        "\n",
        "\n",
        "    candidate_list = filter_bigrams(lemmatized_tokens,pos_tags,window_size,filter_freq)\n",
        "    tester = test_stats_calculator(lemmatized_tokens,window_size)\n",
        "\n",
        "    if test=='t_test':\n",
        "        df = pd.DataFrame(columns=['Bigram', 't-score', 'c(w1w2)','c(w1)','c(w2)'])\n",
        "\n",
        "        for collocation in candidate_list:\n",
        "            new_row = {}\n",
        "            new_row['Bigram'] = str(collocation[0]) + ' ' + str(collocation[1])\n",
        "            new_row['t-score'] = tester.t_dist_test(collocation)[1]\n",
        "            new_row['c(w1w2)'] = tester.coll_counter[collocation]\n",
        "            new_row['c(w1)'] = tester.token_counter[collocation[0]]\n",
        "            new_row['c(w2)'] = tester.token_counter[collocation[1]]\n",
        "\n",
        "            #df = df.append(new_row, ignore_index=True)\n",
        "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
        "\n",
        "        sorted_df = df.sort_values(by='t-score', ascending=False)\n",
        "\n",
        "        return sorted_df.head(k).reset_index(drop=True)\n",
        "\n",
        "    elif test=='chi_test':\n",
        "        df = pd.DataFrame(columns=['Bigram', 'chi-score', 'c(w1w2)','c(w1)','c(w2)'])\n",
        "\n",
        "        for collocation in candidate_list:\n",
        "            new_row = {}\n",
        "            new_row['Bigram'] = str(collocation[0]) + ' ' + str(collocation[1])\n",
        "            new_row['chi-score'] = tester.chi_square_test(collocation)[1]\n",
        "            new_row['c(w1w2)'] = tester.coll_counter[collocation]\n",
        "            new_row['c(w1)'] = tester.token_counter[collocation[0]]\n",
        "            new_row['c(w2)'] = tester.token_counter[collocation[1]]\n",
        "            #df = df.append(new_row, ignore_index=True)\n",
        "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
        "\n",
        "        sorted_df = df.sort_values(by='chi-score', ascending=False)\n",
        "\n",
        "        return sorted_df.head(k).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    elif test=='log_test':\n",
        "        df = pd.DataFrame(columns=['Bigram', 'loglikelihood-score', 'c(w1w2)','c(w1)','c(w2)'])\n",
        "\n",
        "        for collocation in candidate_list:\n",
        "            new_row = {}\n",
        "            new_row['Bigram'] = str(collocation[0]) + ' ' + str(collocation[1])\n",
        "            new_row['loglikelihood-score'] = tester.loglikelihood_test(collocation)[1]\n",
        "            new_row['c(w1w2)'] = tester.coll_counter[collocation]\n",
        "            new_row['c(w1)'] = tester.token_counter[collocation[0]]\n",
        "            new_row['c(w2)'] = tester.token_counter[collocation[1]]\n",
        "            #df = df.append(new_row, ignore_index=True)\n",
        "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
        "\n",
        "        sorted_df = df.sort_values(by='loglikelihood-score', ascending=False)\n",
        "\n",
        "        return sorted_df.head(k).reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        print('test does not exist')\n",
        "        return\n",
        "\n",
        "# top_k = report_topk(20,lemmatized_tokens,pos_tags,'t_test', window_size=1)\n",
        "# print(top_k)\n",
        "\n",
        "# top_k = report_topk(20,lemmatized_tokens,pos_tags,'chi_test', window_size=1)\n",
        "# print(top_k)\n",
        "\n",
        "\"\"\"#Answers\n",
        "\n",
        "part 1a\n",
        "\"\"\"\n",
        "\n",
        "print('Total number of tokens: ',len(tokens))\n",
        "\n",
        "\"\"\"part 1b\"\"\"\n",
        "\n",
        "lemmatized_counter = Counter(lemmatized_tokens)\n",
        "\n",
        "print('Number of \"that\": ', lemmatized_counter['that'] )\n",
        "print('Number of \"the\": ', lemmatized_counter['the'] )\n",
        "print('Number of \"abject\": ', lemmatized_counter['abject'] )\n",
        "print('Number of \"london\": ', lemmatized_counter['london'] )\n",
        "print('Number of \".\": ', lemmatized_counter['.'] )\n",
        "\n",
        "tester1 = test_stats_calculator(lemmatized_tokens,1)\n",
        "print('Number of (“magnificent”,“capital”) with window size 1 =', tester1.coll_counter[('magnificent','capital')])\n",
        "\n",
        "tester3 = test_stats_calculator(lemmatized_tokens,3)\n",
        "print('Number of (“bright”,\"fire\") with window size 3 =', tester3.coll_counter[('bright','fire')])\n",
        "\n",
        "filtered_bigram_list1 = filter_bigrams(lemmatized_tokens,pos_tags,1,10)\n",
        "filtered_bigram_list3 = filter_bigrams(lemmatized_tokens,pos_tags,3,10)\n",
        "\n",
        "\n",
        "\n",
        "print('Is Mr skimpole exist: ', ('mr.', 'skimpole') in filtered_bigram_list1)\n",
        "print('Is spontaneous combustion exist: ', ('spontaneous','combustion') in filtered_bigram_list3)\n",
        "\n",
        "print('Number of occurance of spontaneous combustion:',tester3.coll_counter[('spontaneous','combustion')])\n",
        "\n",
        "\"\"\"part 2\"\"\"\n",
        "\n",
        "top_k_t1 = report_topk(20,lemmatized_tokens,pos_tags,'t_test',window_size = 1)\n",
        "print('t-test with window size 1')\n",
        "print(top_k_t1)\n",
        "\n",
        "top_k_chi1 = report_topk(20,lemmatized_tokens,pos_tags,'chi_test',window_size = 1)\n",
        "print('chi-test with window size 1')\n",
        "print(top_k_chi1)\n",
        "\n",
        "top_k_log1 = report_topk(20,lemmatized_tokens,pos_tags,'log_test',window_size = 1)\n",
        "print('likelihood ratio test with window size 1')\n",
        "print(top_k_log1)\n",
        "\n",
        "top_k_t3 = report_topk(20,lemmatized_tokens,pos_tags,'t_test',window_size = 3)\n",
        "print('t-test with window size 3')\n",
        "print(top_k_t3)\n",
        "\n",
        "top_k_chi3 = report_topk(20,lemmatized_tokens,pos_tags,'chi_test',window_size = 3)\n",
        "print('chi-test with window size 3')\n",
        "print(top_k_chi3)\n",
        "\n",
        "top_k_log3 = report_topk(20,lemmatized_tokens,pos_tags,'log_test',window_size = 3)\n",
        "print('likelihood ratio test with window size 3')\n",
        "print(top_k_log3)\n",
        "\n",
        "\"\"\"Part 3\"\"\"\n",
        "\n",
        "tester1 = test_stats_calculator(lemmatized_tokens,1)\n",
        "\n",
        "print('for Head Clerk')\n",
        "part3coll = ('head', 'clerk')\n",
        "alpha = 0.005\n",
        "coll_bool_t, t_score = tester1.t_dist_test(part3coll,alpha)\n",
        "coll_bool_chi, chi_score = tester1.chi_square_test(part3coll,alpha)\n",
        "coll_bool_log, log_score = tester1.loglikelihood_test(part3coll,alpha)\n",
        "\n",
        "print(\"T-test: \", t_score, 'Collocation?', coll_bool_t)\n",
        "print(\"Chi-square Test: \", chi_score, 'Collocation?', coll_bool_chi)\n",
        "print(\"Likelihood Ratio Test: \", log_score, 'Collocation?', coll_bool_log)\n",
        "\n",
        "print('for great man')\n",
        "part3coll2 = ('great', 'man')\n",
        "alpha = 0.005\n",
        "coll_bool_t, t_score = tester1.t_dist_test(part3coll2,alpha)\n",
        "coll_bool_chi, chi_score = tester1.chi_square_test(part3coll2,alpha)\n",
        "coll_bool_log, log_score = tester1.loglikelihood_test(part3coll2,alpha)\n",
        "\n",
        "print(\"T-test: \", t_score, 'Collocation?', coll_bool_t)\n",
        "print(\"Chi-square Test: \", chi_score, 'Collocation?', coll_bool_chi)\n",
        "print(\"Likelihood Ratio Test: \", log_score, 'Collocation?', coll_bool_log)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Code_Final_NLP_Ass3.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ag0DiEqg3XwnmZ_H6hiyWm7PR4h65TDp\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"NLP_ASS3v5_final.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1nEM3ad9fSTR7cAomsUBSeoRF3Kf2XVhR\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "if 'Fyodor Dostoyevski Processed.txt' not in os.listdir():\n",
        "    print('Please upload the document Fyodor Dostoyevski Processed.txt')\n",
        "    uploaded = files.upload()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt') #required for tokenizer\n",
        "nltk.download('averaged_perceptron_tagger') #required for pos_tag\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "import pandas as pd\n",
        "pd.set_option('display.float_format', '{:.6f}'.format)\n",
        "\n",
        "\n",
        "# Load the stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "\n",
        "\"\"\"Read the txt file and tokenize it with nltk\"\"\"\n",
        "\n",
        "with open('/content/Fyodor Dostoyevski Processed.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "tokens = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "print('number of tokens', np.array(tokens).shape) #total number of tokens\n",
        "\n",
        "\"\"\"POS tags\"\"\"\n",
        "\n",
        "pos_tags = nltk.tag.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "#Example tags\n",
        "print('Example postags', pos_tags[0:10])\n",
        "\n",
        "\"\"\"Custom Lemantizer class\"\"\"\n",
        "\n",
        "class custom_lemmatizer:\n",
        "\n",
        "    tag_dict = {\n",
        "        \"ADJ\": wordnet.ADJ,\n",
        "        \"NOUN\": wordnet.NOUN,\n",
        "        # \"VERB\": wordnet.VERB,\n",
        "        # \"ADV\": wordnet.ADV\n",
        "    }\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    #Retured value is lower case.\n",
        "    def lemmatize(self, word_pos_tuple):\n",
        "        word = word_pos_tuple[0]\n",
        "        pos_tag = word_pos_tuple[1]\n",
        "        if pos_tag in self.tag_dict:\n",
        "            return self.lemmatizer.lemmatize(word, self.tag_dict[pos_tag]).lower()\n",
        "        else:\n",
        "            return word.lower()\n",
        "\n",
        "lemmatizer = custom_lemmatizer()\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(pos_tag) for pos_tag in pos_tags]\n",
        "\n",
        "#obliged mapped to oblige @100\n",
        "print('example lemmatized vs normal:', lemmatized_tokens[100:109], tokens[100:109])\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"#Candidate Collocation Generator\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def filter_bigrams(lemmatized_tokens, pos_tags, window_size,threshold):\n",
        "    filtered_bigrams = []\n",
        "    bigrams = []\n",
        "    for i in range(len(lemmatized_tokens) - 1):\n",
        "        for j in range(window_size):\n",
        "            try:\n",
        "                current_word = lemmatized_tokens[i]\n",
        "                next_word = lemmatized_tokens[i+j + 1]\n",
        "                current_pos_tag = pos_tags[i][1]\n",
        "                next_pos_tag = pos_tags[i+j + 1][1]\n",
        "                #print(current_word,next_word,current_pos_tag,next_pos_tag)\n",
        "\n",
        "                # Check if both words consist only of alphabetical characters and are not stopwords\n",
        "                if current_word.isalpha() and next_word.isalpha() and current_word not in stopwords_list and next_word not in stopwords_list:\n",
        "                    if (current_pos_tag == 'NOUN' and next_pos_tag == 'NOUN') or (current_pos_tag == 'ADJ' and next_pos_tag == 'NOUN'):\n",
        "                        filtered_bigrams.append((current_word, next_word))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "    # bigram_counts = Counter(filtered_bigrams)\n",
        "    # #print(filtered_bigrams)\n",
        "    # for bigram, count in bigram_counts.items():\n",
        "    #     if count >= threshold:\n",
        "    #         bigrams.append(bigram)\n",
        "\n",
        "    collocations = []\n",
        "\n",
        "    for i in range(len(lemmatized_tokens) - 1):\n",
        "        for j in range(window_size):\n",
        "            try:\n",
        "                collocation = (lemmatized_tokens[i], lemmatized_tokens[i+j+1])\n",
        "\n",
        "                collocations.append(collocation)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    bigram_counts = Counter(collocations)\n",
        "    filt_bigram_counts = Counter(filtered_bigrams)\n",
        "    for bigram, count in filt_bigram_counts.items():\n",
        "        if bigram_counts[bigram] >= threshold:\n",
        "            bigrams.append(bigram)\n",
        "\n",
        "\n",
        "\n",
        "    return bigrams\n",
        "\n",
        "filtered_bigram_list1 = filter_bigrams(lemmatized_tokens,pos_tags,1,10)\n",
        "#filtered_bigram_list3 = filter_bigrams(lemmatized_tokens,pos_tags,3,10)\n",
        "\n",
        "\n",
        "\"\"\"#Collecation list\"\"\"\n",
        "\n",
        "def generate_collocations(tokens, window_size):\n",
        "    collocations = []\n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "        for j in range(window_size):\n",
        "            try:\n",
        "                collocation = (tokens[i], tokens[i+j+1])\n",
        "\n",
        "                collocations.append(collocation)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return collocations\n",
        "\n",
        "\"\"\"#Probs\n",
        "\n",
        "in this part marginal probs will be counted as well as bigram probs by just Counts/Total (frequentiest approach)\n",
        "\"\"\"\n",
        "\n",
        "from scipy.stats import t as t_dist\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import chi2, binom\n",
        "\n",
        "class test_stats_calculator:\n",
        "    def __init__(self, tokens, window_size=3):\n",
        "        self.tokens = tokens\n",
        "        self.coll = self.generate_collocations(tokens,window_size)\n",
        "        self.token_counter = Counter(self.tokens)\n",
        "        self.coll_counter = Counter(self.coll)\n",
        "        self.window_size = window_size\n",
        "        self.N = len(tokens)*self.window_size\n",
        "\n",
        "    #Bigram generator.\n",
        "    def generate_collocations(self, tokens, window_size):\n",
        "        collocations = []\n",
        "\n",
        "        for i in range(len(tokens) - 1):\n",
        "            for j in range(window_size):\n",
        "                try:\n",
        "                    collocation = (tokens[i], tokens[i+j+1])\n",
        "\n",
        "                    collocations.append(collocation)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return collocations\n",
        "\n",
        "\n",
        "\n",
        "    def calc_prob(self, itm):\n",
        "\n",
        "        #sample item\n",
        "        if isinstance(itm, str):\n",
        "            #print('Calculating single element probablity...')\n",
        "            if itm in self.token_counter:\n",
        "                return self.token_counter[itm]/sum(self.token_counter.values())\n",
        "            else:\n",
        "                print('Input does not exist in the text!')\n",
        "                return\n",
        "\n",
        "        elif isinstance(itm, tuple):\n",
        "            #print('Calculating collocations probablity...')\n",
        "            #Number of collaction occurance divided by total numbe of tokens.\n",
        "            if itm in self.coll_counter:\n",
        "                return self.coll_counter[itm]/sum(self.coll_counter.values())\n",
        "            else:\n",
        "                print('Input collocation does not exist in the text!')\n",
        "                return\n",
        "\n",
        "        else:\n",
        "            raise TypeError('This type cannot be handeled')\n",
        "\n",
        "\n",
        "    '''\n",
        "    t_distribution will use infinite degree of freedom beacuse N is ver large\n",
        "    Then t dist. will approach to normal distribution. Also due to the lecture\n",
        "    notes I used one sided version.\n",
        "    '''\n",
        "    def t_dist_test(self, collocation, alpha=0.005):\n",
        "        #for H0\n",
        "        prob0 = self.calc_prob(collocation[0])\n",
        "        prob1 = self.calc_prob(collocation[1])\n",
        "\n",
        "        assert prob0 is not None, 'input words does not exist'\n",
        "        assert prob1 is not None, 'input words does not exist'\n",
        "        #H0: items are indipendent\n",
        "        H0 = prob0 * prob1\n",
        "\n",
        "        p = self.calc_prob(collocation)\n",
        "\n",
        "        assert p is not None, 'collocation does not exist'\n",
        "\n",
        "        S2 = p #p*(1-p)\n",
        "\n",
        "        t_val = (p - H0)/math.sqrt(S2/self.N)\n",
        "\n",
        "        if t_val > norm.ppf(1 - alpha):\n",
        "            return True, t_val #It is collocation\n",
        "        else:\n",
        "            return False, t_val\n",
        "\n",
        "\n",
        "\n",
        "    def chi_square_test(self,collocation, alpha=0.005):\n",
        "        observed = np.zeros((2,2))\n",
        "        expected = np.zeros((2,2))\n",
        "\n",
        "        if not isinstance(collocation, tuple):\n",
        "            raise TypeError('This type cannot be handeled')\n",
        "\n",
        "        if collocation not in self.coll_counter:\n",
        "            print('This collocation does not exist')\n",
        "            return 0\n",
        "\n",
        "        #print(self.coll_counter[collocation])\n",
        "        observed[0,0] = self.coll_counter[collocation]\n",
        "        observed[1,0] = self.token_counter[collocation[0]]*self.window_size - observed[0,0]\n",
        "        observed[0,1] = self.token_counter[collocation[1]]*self.window_size - observed[0,0]\n",
        "        observed[1,1] = self.N -  self.token_counter[collocation[0]]*self.window_size - observed[0,1]\n",
        "\n",
        "\n",
        "        #Generic part did not worked well\n",
        "        # expected[0,0] = (self.token_counter[collocation[0]]* self.token_counter[collocation[1]])/self.N\n",
        "        # expected[1,0] = (self.token_counter[collocation[0]]* (self.N - self.token_counter[collocation[1]]))/self.N\n",
        "        # expected[0,1] = ((self.N - self.token_counter[collocation[0]])* (self.token_counter[collocation[1]]))/self.N\n",
        "        # expected[1,1] = ((self.N - self.token_counter[collocation[0]])* (self.N - self.token_counter[collocation[1]]))/self.N\n",
        "\n",
        "\n",
        "\n",
        "        # expected = expected + 1 #add one smoothing\n",
        "        # chi_val = 0\n",
        "\n",
        "        # for i in range(observed.shape[0]):\n",
        "        #     for j in range(observed.shape[1]):\n",
        "        #         #print((observed[i,j], expected[i,j]),chi_val)\n",
        "        #         chi_val += (observed[i,j] - expected[i,j])**2 /expected[i,j]\n",
        "\n",
        "        #Shorcut worked better\n",
        "        chi_val = (self.N*(observed[0,0]*observed[1,1]-observed[0,1]*observed[1,0])**2)/(\n",
        "            (observed[0,0]+observed[0,1])*(observed[0,0]+observed[1,0])*\n",
        "             (observed[0,1]+observed[1,1])*(observed[1,0]+observed[1,1]))\n",
        "\n",
        "        df = (observed.shape[0] - 1)*(observed.shape[1] - 1)\n",
        "\n",
        "        if chi_val > chi2.ppf(1 - alpha,df):\n",
        "            return True, chi_val #It is collocation\n",
        "        else:\n",
        "            return False, chi_val\n",
        "\n",
        "\n",
        "\n",
        "    def loglikelihood_test(self,collocation, alpha=0.005):\n",
        "        eps = 5e-324 #math.ulp(0.0) value\n",
        "        c1 = self.token_counter[collocation[0]]*self.window_size\n",
        "        c2 = self.token_counter[collocation[1]]*self.window_size\n",
        "        c12 = self.coll_counter[collocation]\n",
        "        p = c2/self.N\n",
        "        p1 = c12/(c1)\n",
        "        p2 = (c2 - c12) / (self.N-c1)\n",
        "\n",
        "        LH1 = (binom.pmf(c12,c1,p))*(binom.pmf(c2-c12,self.N-c1,p))\n",
        "        LH2 = (binom.pmf(c12,c1,p1))*(binom.pmf(c2-c12,self.N-c1,p2))\n",
        "\n",
        "        if LH1 == 0:\n",
        "            LH1 = eps\n",
        "        if LH2 == 0:\n",
        "            LH2 = eps\n",
        "\n",
        "        log_val = -2*np.log((LH1/LH2))\n",
        "\n",
        "        if log_val > chi2.ppf(1 - alpha,1):\n",
        "            return True, log_val #It is collocation\n",
        "        else:\n",
        "            return False, log_val\n",
        "\n",
        "\"\"\"#Report top k\"\"\"\n",
        "\n",
        "def report_topk(k, lemmatized_tokens, pos_tags, test='t_test', window_size = 3, filter_freq=10, ):\n",
        "\n",
        "\n",
        "    candidate_list = filter_bigrams(lemmatized_tokens,pos_tags,window_size,filter_freq)\n",
        "    tester = test_stats_calculator(lemmatized_tokens,window_size)\n",
        "\n",
        "    if test=='t_test':\n",
        "        df = pd.DataFrame(columns=['Bigram', 't-score', 'c(w1w2)','c(w1)','c(w2)'])\n",
        "\n",
        "        for collocation in candidate_list:\n",
        "            new_row = {}\n",
        "            new_row['Bigram'] = str(collocation[0]) + ' ' + str(collocation[1])\n",
        "            new_row['t-score'] = tester.t_dist_test(collocation)[1]\n",
        "            new_row['c(w1w2)'] = tester.coll_counter[collocation]\n",
        "            new_row['c(w1)'] = tester.token_counter[collocation[0]]\n",
        "            new_row['c(w2)'] = tester.token_counter[collocation[1]]\n",
        "\n",
        "            #df = df.append(new_row, ignore_index=True)\n",
        "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
        "\n",
        "        sorted_df = df.sort_values(by='t-score', ascending=False)\n",
        "\n",
        "        return sorted_df.head(k).reset_index(drop=True)\n",
        "\n",
        "    elif test=='chi_test':\n",
        "        df = pd.DataFrame(columns=['Bigram', 'chi-score', 'c(w1w2)','c(w1)','c(w2)'])\n",
        "\n",
        "        for collocation in candidate_list:\n",
        "            new_row = {}\n",
        "            new_row['Bigram'] = str(collocation[0]) + ' ' + str(collocation[1])\n",
        "            new_row['chi-score'] = tester.chi_square_test(collocation)[1]\n",
        "            new_row['c(w1w2)'] = tester.coll_counter[collocation]\n",
        "            new_row['c(w1)'] = tester.token_counter[collocation[0]]\n",
        "            new_row['c(w2)'] = tester.token_counter[collocation[1]]\n",
        "            #df = df.append(new_row, ignore_index=True)\n",
        "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
        "\n",
        "        sorted_df = df.sort_values(by='chi-score', ascending=False)\n",
        "\n",
        "        return sorted_df.head(k).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    elif test=='log_test':\n",
        "        df = pd.DataFrame(columns=['Bigram', 'loglikelihood-score', 'c(w1w2)','c(w1)','c(w2)'])\n",
        "\n",
        "        for collocation in candidate_list:\n",
        "            new_row = {}\n",
        "            new_row['Bigram'] = str(collocation[0]) + ' ' + str(collocation[1])\n",
        "            new_row['loglikelihood-score'] = tester.loglikelihood_test(collocation)[1]\n",
        "            new_row['c(w1w2)'] = tester.coll_counter[collocation]\n",
        "            new_row['c(w1)'] = tester.token_counter[collocation[0]]\n",
        "            new_row['c(w2)'] = tester.token_counter[collocation[1]]\n",
        "            #df = df.append(new_row, ignore_index=True)\n",
        "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
        "\n",
        "        sorted_df = df.sort_values(by='loglikelihood-score', ascending=False)\n",
        "\n",
        "        return sorted_df.head(k).reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        print('test does not exist')\n",
        "        return\n",
        "\n",
        "# top_k = report_topk(20,lemmatized_tokens,pos_tags,'t_test', window_size=1)\n",
        "# print(top_k)\n",
        "\n",
        "# top_k = report_topk(20,lemmatized_tokens,pos_tags,'chi_test', window_size=1)\n",
        "# print(top_k)\n",
        "\n",
        "\"\"\"#Answers\n",
        "\n",
        "part 1a\n",
        "\"\"\"\n",
        "\n",
        "print('Total number of tokens: ',len(tokens))\n",
        "\n",
        "\"\"\"part 1b\"\"\"\n",
        "\n",
        "lemmatized_counter = Counter(lemmatized_tokens)\n",
        "\n",
        "print('Number of \"that\": ', lemmatized_counter['that'] )\n",
        "print('Number of \"the\": ', lemmatized_counter['the'] )\n",
        "print('Number of \"abject\": ', lemmatized_counter['abject'] )\n",
        "print('Number of \"london\": ', lemmatized_counter['london'] )\n",
        "print('Number of \".\": ', lemmatized_counter['.'] )\n",
        "\n",
        "tester1 = test_stats_calculator(lemmatized_tokens,1)\n",
        "print('Number of (“magnificent”,“capital”) with window size 1 =', tester1.coll_counter[('magnificent','capital')])\n",
        "\n",
        "tester3 = test_stats_calculator(lemmatized_tokens,3)\n",
        "print('Number of (“bright”,\"fire\") with window size 3 =', tester3.coll_counter[('bright','fire')])\n",
        "\n",
        "filtered_bigram_list1 = filter_bigrams(lemmatized_tokens,pos_tags,1,10)\n",
        "filtered_bigram_list3 = filter_bigrams(lemmatized_tokens,pos_tags,3,10)\n",
        "\n",
        "\n",
        "\n",
        "print('Is Mr skimpole exist: ', ('mr.', 'skimpole') in filtered_bigram_list1)\n",
        "print('Is spontaneous combustion exist: ', ('spontaneous','combustion') in filtered_bigram_list3)\n",
        "\n",
        "print('Number of occurance of spontaneous combustion:',tester3.coll_counter[('spontaneous','combustion')])\n",
        "\n",
        "\"\"\"part 2\"\"\"\n",
        "\n",
        "top_k_t1 = report_topk(20,lemmatized_tokens,pos_tags,'t_test',window_size = 1)\n",
        "print('t-test with window size 1')\n",
        "print(top_k_t1)\n",
        "\n",
        "top_k_chi1 = report_topk(20,lemmatized_tokens,pos_tags,'chi_test',window_size = 1)\n",
        "print('chi-test with window size 1')\n",
        "print(top_k_chi1)\n",
        "\n",
        "top_k_log1 = report_topk(20,lemmatized_tokens,pos_tags,'log_test',window_size = 1)\n",
        "print('likelihood ratio test with window size 1')\n",
        "print(top_k_log1)\n",
        "\n",
        "top_k_t3 = report_topk(20,lemmatized_tokens,pos_tags,'t_test',window_size = 3)\n",
        "print('t-test with window size 3')\n",
        "print(top_k_t3)\n",
        "\n",
        "top_k_chi3 = report_topk(20,lemmatized_tokens,pos_tags,'chi_test',window_size = 3)\n",
        "print('chi-test with window size 3')\n",
        "print(top_k_chi3)\n",
        "\n",
        "top_k_log3 = report_topk(20,lemmatized_tokens,pos_tags,'log_test',window_size = 3)\n",
        "print('likelihood ratio test with window size 3')\n",
        "print(top_k_log3)\n",
        "\n",
        "\"\"\"Part 3\"\"\"\n",
        "\n",
        "tester1 = test_stats_calculator(lemmatized_tokens,1)\n",
        "\n",
        "print('for Head Clerk')\n",
        "part3coll = ('head', 'clerk')\n",
        "alpha = 0.005\n",
        "coll_bool_t, t_score = tester1.t_dist_test(part3coll,alpha)\n",
        "coll_bool_chi, chi_score = tester1.chi_square_test(part3coll,alpha)\n",
        "coll_bool_log, log_score = tester1.loglikelihood_test(part3coll,alpha)\n",
        "\n",
        "print(\"T-test: \", t_score, 'Collocation?', coll_bool_t)\n",
        "print(\"Chi-square Test: \", chi_score, 'Collocation?', coll_bool_chi)\n",
        "print(\"Likelihood Ratio Test: \", log_score, 'Collocation?', coll_bool_log)\n",
        "\n",
        "print('for great man')\n",
        "part3coll2 = ('great', 'man')\n",
        "alpha = 0.005\n",
        "coll_bool_t, t_score = tester1.t_dist_test(part3coll2,alpha)\n",
        "coll_bool_chi, chi_score = tester1.chi_square_test(part3coll2,alpha)\n",
        "coll_bool_log, log_score = tester1.loglikelihood_test(part3coll2,alpha)\n",
        "\n",
        "print(\"T-test: \", t_score, 'Collocation?', coll_bool_t)\n",
        "print(\"Chi-square Test: \", chi_score, 'Collocation?', coll_bool_chi)\n",
        "print(\"Likelihood Ratio Test: \", log_score, 'Collocation?', coll_bool_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t48S9vMoaJuj",
        "outputId": "d04527fb-3304-4493-c5fb-6046aa929598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of tokens (1425758,)\n",
            "Example postags [('part', 'NOUN'), ('i', 'VERB'), ('chapter', 'NOUN'), ('i', 'NOUN'), ('on', 'ADP'), ('an', 'DET'), ('exceptionally', 'ADV'), ('hot', 'ADJ'), ('evening', 'VERB'), ('early', 'ADJ')]\n",
            "example lemmatized vs normal: ['obliged', 'to', 'pass', 'her', 'kitchen', ',', 'the', 'door', 'of'] ['obliged', 'to', 'pass', 'her', 'kitchen', ',', 'the', 'door', 'of']\n",
            "Total number of tokens:  1425758\n",
            "Number of \"that\":  19429\n",
            "Number of \"the\":  48392\n",
            "Number of \"abject\":  21\n",
            "Number of \"london\":  2\n",
            "Number of \".\":  51738\n",
            "Number of (“magnificent”,“capital”) with window size 1 = 1\n",
            "Number of (“bright”,\"fire\") with window size 3 = 1\n",
            "Is Mr skimpole exist:  False\n",
            "Is spontaneous combustion exist:  False\n",
            "Number of occurance of spontaneous combustion: 1\n",
            "t-test with window size 1\n",
            "                     Bigram   t-score c(w1w2) c(w1) c(w2)\n",
            "0       stepan trofimovitch 22.619077     512   525   513\n",
            "1        pyotr stepanovitch 22.547839     509   834   509\n",
            "2          varvara petrovna 20.534441     422   474   507\n",
            "3         katerina ivanovna 20.239072     410   427   635\n",
            "4   nikolay vsyevolodovitch 17.657110     312   518   312\n",
            "5         fyodor pavlovitch 17.052928     291   306   461\n",
            "6                   old man 16.857569     289  1356  2546\n",
            "7       nastasia philipovna 15.583753     243   417   251\n",
            "8                 young man 15.140575     232   776  2546\n",
            "9                 old woman 14.353166     208  1356  1047\n",
            "10         yulia mihailovna 14.175303     201   215   202\n",
            "11         pyotr petrovitch 13.100118     172   834   331\n",
            "12    lizabetha prokofievna 13.074945     171   185   177\n",
            "13               great deal 12.790651     164  1202   237\n",
            "14      dmitri fyodorovitch 12.720232     162   427   327\n",
            "15       evgenie pavlovitch 12.563970     158   227   461\n",
            "16          thousand rouble 11.980517     144   614   543\n",
            "17                long time 11.793035     143  1074  2623\n",
            "18     mavriky nikolaevitch 11.487929     132   149   132\n",
            "19               first time 11.192482     130  1297  2623\n",
            "chi-test with window size 1\n",
            "                    Bigram      chi-score c(w1w2) c(w1) c(w2)\n",
            "0      stepan trofimovitch 1387729.399419     512   525   513\n",
            "1     ippolit kirillovitch 1359441.767387      41    43    41\n",
            "2        lef nicolaievitch 1359441.767387      41    43    41\n",
            "3        avdotya romanovna 1341883.293600     112   119   112\n",
            "4         yulia mihailovna 1326305.255085     201   215   202\n",
            "5          nikodim fomitch 1316082.461507      24    24    26\n",
            "6    lizabetha prokofievna 1273170.745295     171   185   177\n",
            "7     mavriky nikolaevitch 1263072.562364     132   149   132\n",
            "8      trifon borissovitch 1235651.733191      39    45    39\n",
            "9       rodion romanovitch 1205267.277621      82    97    82\n",
            "10      mihail makarovitch 1197633.359951      21    25    21\n",
            "11  gavrila ardalionovitch 1173527.381984      58    61    67\n",
            "12        arina prohorovna 1120124.358215      39    44    44\n",
            "13        varvara petrovna 1056419.206291     422   474   507\n",
            "14     semyon yakovlevitch 1034363.293854      37    51    37\n",
            "15          kuzma kuzmitch  983275.172327      20    29    20\n",
            "16        daria alexeyevna  980371.988481      19    21    25\n",
            "17          darya pavlovna  935805.629431      49    62    59\n",
            "18       katerina ivanovna  883756.256081     410   427   635\n",
            "19      pyotr stepanovitch  869958.438755     509   834   509\n",
            "likelihood ratio test with window size 1\n",
            "                     Bigram  loglikelihood-score c(w1w2) c(w1) c(w2)\n",
            "0         avdotya romanovna          1484.988324     112   119   112\n",
            "1        rodion romanovitch          1484.485695      82    97    82\n",
            "2      mavriky nikolaevitch          1484.274974     132   149   132\n",
            "3        andrey antonovitch          1483.464043      83   142    83\n",
            "4                 de griers          1482.991266      95   243    95\n",
            "5      nikolay parfenovitch          1482.609155     103   518   103\n",
            "6       stepan trofimovitch          1482.524036     512   525   513\n",
            "7          yulia mihailovna          1482.442392     201   215   202\n",
            "8   nikolay vsyevolodovitch          1482.215735     312   518   312\n",
            "9        pyotr stepanovitch          1481.769448     509   834   509\n",
            "10    lizabetha prokofievna          1480.829440     171   185   177\n",
            "11   pulcheria alexandrovna          1480.272014     123   124   242\n",
            "12          madame hohlakov          1479.994841      90   247    93\n",
            "13      nastasia philipovna          1478.483150     243   417   251\n",
            "14        fyodor pavlovitch          1477.400558     291   306   461\n",
            "15        katerina ivanovna          1476.984074     410   427   635\n",
            "16         varvara petrovna          1476.922372     422   474   507\n",
            "17               great deal          1475.957207     164  1202   237\n",
            "18      alexey fyodorovitch          1475.772277     106   226   327\n",
            "19       evgenie pavlovitch          1475.616140     158   227   461\n",
            "t-test with window size 3\n",
            "                     Bigram   t-score c(w1w2) c(w1) c(w2)\n",
            "0       stepan trofimovitch 22.602388     512   525   513\n",
            "1        pyotr stepanovitch 22.521453     509   834   509\n",
            "2          varvara petrovna 20.518038     422   474   507\n",
            "3         katerina ivanovna 20.220295     410   427   635\n",
            "4   nikolay vsyevolodovitch 17.644282     312   518   312\n",
            "5         fyodor pavlovitch 17.041334     291   306   461\n",
            "6                   old man 16.602824     290  1356  2546\n",
            "7       nastasia philipovna 15.574340     243   417   251\n",
            "8                 young man 15.025308     234   776  2546\n",
            "9                 old woman 14.459155     215  1356  1047\n",
            "10         yulia mihailovna 14.171011     201   215   202\n",
            "11    lizabetha prokofievna 13.071437     171   185   177\n",
            "12         pyotr petrovitch 13.070596     172   834   331\n",
            "13               great deal 12.798577     165  1202   237\n",
            "14      dmitri fyodorovitch 12.704848     162   427   327\n",
            "15       evgenie pavlovitch 12.552296     158   227   461\n",
            "16                    ha ha 12.519309     157   252   252\n",
            "17          thousand rouble 12.473985     157   614   543\n",
            "18           hundred rouble 12.329791     153   428   543\n",
            "19                   let go 11.689340     145  1085  1858\n",
            "chi-test with window size 3\n",
            "                    Bigram     chi-score c(w1w2) c(w1) c(w2)\n",
            "0      stepan trofimovitch 461893.806629     512   525   513\n",
            "1        lef nicolaievitch 453092.589206      41    43    41\n",
            "2     ippolit kirillovitch 453092.589206      41    43    41\n",
            "3        avdotya romanovna 447145.098601     112   119   112\n",
            "4         yulia mihailovna 441833.754547     201   215   202\n",
            "5          nikodim fomitch 438662.153881      24    24    26\n",
            "6    lizabetha prokofievna 424162.251792     171   185   177\n",
            "7     mavriky nikolaevitch 420848.189555     132   149   132\n",
            "8      trifon borissovitch 411831.911283      39    45    39\n",
            "9       rodion romanovitch 401646.427025      82    97    82\n",
            "10      mihail makarovitch 399183.120062      21    25    21\n",
            "11  gavrila ardalionovitch 391098.461363      58    61    67\n",
            "12        arina prohorovna 373322.786483      39    44    44\n",
            "13        varvara petrovna 351577.131206     422   474   507\n",
            "14     semyon yakovlevitch 344738.431769      37    51    37\n",
            "15          kuzma kuzmitch 327731.724277      20    29    20\n",
            "16        daria alexeyevna 326765.329658      19    21    25\n",
            "17                wisp tow 323415.555632      14    18    16\n",
            "18          darya pavlovna 311869.877774      49    62    59\n",
            "19       katerina ivanovna 294038.852117     410   427   635\n",
            "likelihood ratio test with window size 3\n",
            "                     Bigram  loglikelihood-score c(w1w2) c(w1) c(w2)\n",
            "0         avdotya romanovna          1475.449377     112   119   112\n",
            "1      mavriky nikolaevitch          1475.092831     132   149   132\n",
            "2     lizabetha prokofievna          1474.542984     171   185   177\n",
            "3    pulcheria alexandrovna          1474.390261     123   124   242\n",
            "4          yulia mihailovna          1474.269767     201   215   202\n",
            "5                     ha ha          1473.984641     157   252   252\n",
            "6                great deal          1473.841844     165  1202   237\n",
            "7       nastasia philipovna          1473.692388     243   417   251\n",
            "8       dmitri fyodorovitch          1473.543022     162   427   327\n",
            "9          pyotr petrovitch          1473.416282     172   834   331\n",
            "10       evgenie pavlovitch          1473.293795     158   227   461\n",
            "11  nikolay vsyevolodovitch          1473.248923     312   518   312\n",
            "12           hundred rouble          1473.002539     153   428   543\n",
            "13        fyodor pavlovitch          1472.915994     291   306   461\n",
            "14         varvara petrovna          1472.508886     422   474   507\n",
            "15      stepan trofimovitch          1472.424324     512   525   513\n",
            "16       pyotr stepanovitch          1472.273630     509   834   509\n",
            "17        katerina ivanovna          1472.263746     410   427   635\n",
            "18                  old man          1470.707417     290  1356  2546\n",
            "19                young man          1457.108826     234   776  2546\n",
            "for Head Clerk\n",
            "T-test:  4.674127666133668 Collocation? True\n",
            "Chi-square Test:  6294.821125652069 Collocation? True\n",
            "Likelihood Ratio Test:  209.66265578477234 Collocation? True\n",
            "for great man\n",
            "T-test:  3.73672357664933 Collocation? True\n",
            "Chi-square Test:  117.4030889458421 Collocation? True\n",
            "Likelihood Ratio Test:  45.15878843720214 Collocation? True\n"
          ]
        }
      ]
    }
  ]
}